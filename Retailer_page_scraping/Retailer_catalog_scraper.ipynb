{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multithreading scraper with using selenium and scrapy in Chrome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download libraries\n",
    "# scraping libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "# multithreading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# other\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "import scrapy\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from datetime import date\n",
    "import datetime\n",
    "from IPython.display import display\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "#for database connection\n",
    "from dataclasses import dataclass\n",
    "from sqlalchemy import create_engine, event, DateTime, Column, String, MetaData, Integer, \\\n",
    "    Binary, PrimaryKeyConstraint, Date\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker, Session\n",
    "from contextlib import contextmanager\n",
    "from urllib.parse import quote_plus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_driver(chromedriver_path: str) -> webdriver.Chrome:\n",
    "    \"\"\"Returns Chromedriver with specified parameters\"\"\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    \n",
    "    # headless-mode activation\n",
    "    options.headless = True\n",
    "    options.add_argument('window-size=1920x1080')\n",
    "    ###############################################\n",
    "    \n",
    "    options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    options.add_experimental_option('useAutomationExtension', False)\n",
    "\n",
    "    #fix handshake failed\n",
    "    options.add_argument(\"--ignore-certificate-error\")\n",
    "    options.add_argument(\"--ignore-ssl-errors\")\n",
    "    caps = webdriver.DesiredCapabilities.CHROME.copy()\n",
    "    caps['acceptInsecureCerts'] = True\n",
    "    caps['acceptSslCerts'] = True\n",
    "    \n",
    "    #add user-agent to avoid blocking\n",
    "    #create random user-agents\n",
    "    ua = UserAgent()\n",
    "    userAgent = ua.random\n",
    "    \n",
    "    #add user-agent to options\n",
    "    options.add_argument(f'user-agent={userAgent}')\n",
    "\n",
    "    \n",
    "    #set driver parametres\n",
    "    driver = webdriver.Chrome(options=options,\n",
    "                              executable_path=chromedriver_path,\n",
    "                              desired_capabilities=caps)\n",
    "    \n",
    "    # uncomment in non-headless mode\n",
    "    #driver.maximize_window()\n",
    "    \n",
    "    driver.implicitly_wait(15)\n",
    "    return driver\n",
    "\n",
    "    \n",
    "def choose_city(city_name: str, driver: webdriver.Chrome) -> None:\n",
    "    \"\"\"Chooses the necessary city\"\"\"\n",
    "    if city_name == \"Москва\":\n",
    "        driver.find_element_by_xpath('//*[contains(text(), \"Да, все верно\")]').click()\n",
    "        time.sleep(np.random.choice(delays))\n",
    "    else:\n",
    "        driver.find_element_by_xpath('//*[contains(text(), \"Выбрать другой\")]').click()\n",
    "        time.sleep(np.random.choice(delays))\n",
    "\n",
    "        # push choose city button\n",
    "        driver.find_element_by_xpath(f'//*[contains(text(), \"{city_name}\")]').click()\n",
    "        time.sleep(np.random.choice(delays))\n",
    "\n",
    "        \n",
    "def choose_store(store: str, driver: webdriver.Chrome) -> None:\n",
    "    \"\"\"Chooses the necessary store address\"\"\"\n",
    "    try:\n",
    "        #open the choose store window\n",
    "        driver.find_element_by_xpath('//*[contains(text(), \"Выбрать другой\")]').click()\n",
    "        time.sleep(np.random.choice(delays))\n",
    " \n",
    "        #find address\n",
    "        driver.find_element_by_xpath(f'//div[@class=\"store-item__content\"]/div[contains(text(), \"{store}\")]').click()\n",
    "        time.sleep(np.random.choice(delays))\n",
    "        \n",
    "        #confirm selection\n",
    "        driver.find_element_by_xpath(f'//span[contains(text(), \"Выбрать магазин\")]').click()\n",
    "        time.sleep(np.random.choice(delays))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print('Error in choosing the store')\n",
    "        print(e)\n",
    "    \n",
    "    \n",
    "def close_cookie_window(driver: webdriver.Chrome) -> None:\n",
    "    '''Closes the cookie window'''\n",
    "    try:\n",
    "        driver.find_element_by_xpath('//div[@class=\"button__inner cookie-usage-notice__button-inner--desktop\"]').click()\n",
    "        time.sleep(np.random.choice(delays))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "def load_full_page(driver: webdriver.Chrome) -> None:\n",
    "    \"\"\"Clicks 'Загрузить ещё' button as many times as needed to load full page\"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            button_element = driver.find_element_by_xpath(\"//*[contains(text(), 'Загрузить ещё')]\")\n",
    "            button_element.click()\n",
    "            time.sleep(1)\n",
    "        except:\n",
    "            try:\n",
    "                # print('1st Error in full page loading')\n",
    "                button_element = driver.find_element_by_xpath(\"//*[contains(text(), 'Загрузить ещё')]\")\n",
    "                button_element.click()\n",
    "                time.sleep(1)\n",
    "            except:\n",
    "                pass\n",
    "            # print('Page downloaded')\n",
    "            break\n",
    "\n",
    "def get_categories_and_links(driver: webdriver.Chrome) -> None:\n",
    "    \"\"\"Gets category name and link from categories page\"\"\"\n",
    "    try:\n",
    "        find_element_by_xpath('/html/body/div[1]/div[1]/div/main/article/div/div/div/div[2]/a')\n",
    "        all_types_element = driver.find_elements_by_xpath('/html/body/div[1]/div[1]/div/main/article/div/div/div/div[2]/a')\n",
    "    except:\n",
    "        #if there is a discount window on the page\n",
    "        all_types_element = driver.find_elements_by_xpath('/html/body/div[1]/div[1]/div/main/article/div/div/div/div[3]/a')\n",
    "    \n",
    "    all_types = [(prod_type.text, prod_type.get_attribute('href')) for prod_type in all_types_element]\n",
    "    \n",
    "    return all_types\n",
    "\n",
    "def get_cities_categories(inp: tuple) -> None:\n",
    "    \"\"\"Gets category name and link from categories page for several cities\"\"\"\n",
    "    city, store = inp #city and store address as input\n",
    "    \n",
    "    # open browser\n",
    "    driver = get_driver(chromedriver_path='/Program Files (x86)/chromedriver')\n",
    "    \n",
    "    # go to catalog page\n",
    "    driver.get(categories_link)\n",
    "    \n",
    "    time.sleep(np.random.choice(delays))\n",
    "    \n",
    "    # Choose the necessary location\n",
    "    # Choose city\n",
    "    choose_city(city, driver)\n",
    "\n",
    "    # Choose store\n",
    "    choose_store(store, driver)\n",
    "\n",
    "    # Close cookie window\n",
    "    close_cookie_window(driver)\n",
    "\n",
    "    # Get categories\n",
    "    all_types = get_categories_and_links(driver)\n",
    "    \n",
    "    # Add city and store to categories and link\n",
    "    all_product_links = [(prod_name, prod_link, city, store) for prod_name, prod_link in all_types]\n",
    "    \n",
    "    stores_categories.append(all_product_links)\n",
    "\n",
    "    print(city, store,'finished with',len(all_product_links),'categories')\n",
    "    \n",
    "    # close browser    \n",
    "    driver.quit()\n",
    "\n",
    "def collect_data(html_str: str, xpaths: dict, prod_type: str, city: str, store: str) -> pd.DataFrame:\n",
    "    \"\"\"Collect the necessary data for each product on page\"\"\"\n",
    "    selector = scrapy.Selector(text=html_str)\n",
    "    products = selector.xpath(\"//div[@class='catalog-grid-container__grid']/div[@class='sku-card-small-container']\")\n",
    "    \n",
    "    products_data = []\n",
    "\n",
    "    for product in products:\n",
    "        d = {}\n",
    "        for name, xpath in xpaths.items():\n",
    "            try:\n",
    "                d[name] = product.xpath(xpath).get()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        if 'links' in d and d['links']:\n",
    "            #d['links'] = 'link' + d['links']\n",
    "\n",
    "        if 'item' in d and d['item']:\n",
    "            d['item'] = d['item'].split('-')[-1][: -1]\n",
    "\n",
    "        products_data.append(d)\n",
    "    \n",
    "    df = pd.DataFrame(products_data)\n",
    "    \n",
    "    #add category\n",
    "    df['category'] = prod_type\n",
    "    \n",
    "    #add city\n",
    "    df['city'] = city\n",
    "\n",
    "    #add store\n",
    "    df['store'] = store\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_data(inp: tuple) -> None: #  city: str, store: str\n",
    "    '''Gets products data for each category'''\n",
    "    prod_type, link, city, store = inp\n",
    "\n",
    "    # open browser\n",
    "    driver = get_driver(chromedriver_path='/Program Files (x86)/chromedriver')\n",
    "\n",
    "    # go to catalog page\n",
    "    driver.get(link)\n",
    "    time.sleep(np.random.choice(delays))\n",
    "    \n",
    "    # Choose location\n",
    "    # Choose city\n",
    "    choose_city(city, driver)\n",
    "    \n",
    "    # Choose store\n",
    "    choose_store(store, driver)\n",
    "\n",
    "    # Close cookie\n",
    "    close_cookie_window(driver)\n",
    "    \n",
    "    # Load full page\n",
    "    load_full_page(driver)\n",
    "            \n",
    "    # Get necessary data by xpaths\n",
    "    xpaths = {'name': './/div[@class=\"sku-card-small__title\"]//text()',\n",
    "             'links': './a/@href',\n",
    "             'item': './a/@href',\n",
    "             'prices_int_reg': './/div[contains(@class, \"sku-price--regular\")]/span[@class=\"sku-price__integer\"]/text()',\n",
    "             'prices_fract_reg': './/div[contains(@class, \"sku-price--regular\")]/small[@class=\"sku-price__fraction\"]/text()',\n",
    "             'prices_int_disc': './/div[contains(@class, \"sku-price--primary\")]/span[@class=\"sku-price__integer\"]/text()',\n",
    "             'prices_fract_disc': './/div[contains(@class, \"sku-price--primary\")]/small[@class=\"sku-price__fraction\"]/text()',\n",
    "             'aktion_mark': './/div[contains(@class, \"sku-card-small__discount-label\")]/text()'}\n",
    "\n",
    "    df = collect_data(driver.page_source, xpaths, prod_type, city, store)\n",
    "    \n",
    "    # close browser\n",
    "    driver.quit()\n",
    "    \n",
    "    \n",
    "    whole_data.append(df)\n",
    "    \n",
    "    print(f'Finished {prod_type} {city} {store} with {df.shape[0]} products')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting parametres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of stores addresses\n",
    "#stores_list = [('city','store')]\n",
    "\n",
    "#catalog page link\n",
    "#categories_link = 'link'\n",
    "\n",
    "#list of random delays\n",
    "delays = [3,4,5,6,7,8,9,10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting categories for each store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get categories\n",
    "stores_categories = []\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    for store_data in stores_list:\n",
    "        executor.submit(get_cities_categories, store_data)\n",
    "\n",
    "#transformation\n",
    "stores_categories = list(itertools.chain(*stores_categories))\n",
    "        \n",
    "#time stats   \n",
    "finish = datetime.datetime.now()\n",
    "print(\"Download complete. Passed: \", (finish - start).seconds//3600, \"hours \", ((finish - start).seconds//60)%60, \"minutes\")\n",
    "print()\n",
    "\n",
    "print(stores_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting product data for each category in each store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "whole_data = []\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    for product_data in stores_categories:\n",
    "        executor.submit(get_data, product_data)\n",
    "        \n",
    "#stats    \n",
    "finish = datetime.datetime.now()\n",
    "print(\"Download complete. Passed: \", (finish - start).seconds//3600, \"hours \", ((finish - start).seconds//60)%60, \"minutes\")\n",
    "print('Categories: ',len(whole_data), 'of', len(stores_categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#make dataframe\n",
    "df = pd.concat(whole_data, axis=0).reset_index(drop=True)\n",
    "#df = resp\n",
    "\n",
    "\n",
    "#drop garbage from prices\n",
    "df['prices_int_reg'] = df['prices_int_reg'].apply(lambda x: re.sub(re.compile(r'\\s+'), '', x))\n",
    "df['prices_int_disc'] = df['prices_int_disc'].apply(lambda x: re.sub(re.compile(r'\\s+'), '', x))\n",
    "\n",
    "#create regular price\n",
    "#regular price\n",
    "df['regular_price'] = df['prices_int_reg'].str.cat(df['prices_fract_reg'].values,sep='.')\n",
    "df['regular_price'] = df['regular_price'].astype(float)\n",
    "\n",
    "#discount price\n",
    "df['promo_price'] = df['prices_int_disc'].str.cat(df['prices_fract_disc'].values,sep='.')\n",
    "df['promo_price'] = df['promo_price'].astype(float)\n",
    "\n",
    "#garbage out\n",
    "df = df.drop(['prices_int_reg','prices_fract_reg','prices_int_disc','prices_fract_disc'], 1)\n",
    "\n",
    "#make action mark\n",
    "df['aktion'] = df['aktion_mark'].apply(lambda x: \"Yes\" if x != '' else '')\n",
    "df = df.drop('aktion_mark', 1)\n",
    "\n",
    "#add download date\n",
    "df['download_date'] = date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "#reorder columns\n",
    "df = df[['item', 'name', 'regular_price', 'promo_price','aktion', 'category', 'download_date', 'city', 'store', 'links']]\n",
    "\n",
    "#create store id\n",
    "df['store_id'] = df['city'].map(str) + '_' + df['store'].map(str)\n",
    "\n",
    "#save data\n",
    "for store_id in df['store_id'].unique():\n",
    "    df_temp = df.query('store_id == @store_id')\n",
    "    df_temp.drop('store_id', axis = 1).to_excel(store_id + '_' + datetime.datetime.now().strftime(\"%B%d\") + '_full' +'.xlsx'\n",
    "            , encoding =\"UTF-8\",index=False)\n",
    "print('data saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database connection\n",
    "\n",
    "Add parsing results into database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection parametres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@dataclass(frozen=True)\n",
    "#class DbConfig:\n",
    "#    db_server = server address\n",
    "#    pwd = password\n",
    "#    uid = user id\n",
    "#    db_name = database name\n",
    "#    driver = driver\n",
    "#    params = quote_plus(\n",
    "#       'DRIVER={DRIVER};SERVER={DB_SERVER};DATABASE={DB_NAME};UID={UID};PWD={PWD}'.format(\n",
    "#           DB_SERVER=db_server, DB_NAME=db_name, UID=uid, PWD=pwd, DRIVER=driver, \n",
    "#        ))\n",
    "#    print(params)\n",
    "#    conn_str = 'mssql+pyodbc:///?odbc_connect={}'.format(params)\n",
    "\n",
    "\n",
    "#class Db:\n",
    "#    def __init__(self):\n",
    "#        self._db_conf = DbConfig()\n",
    "#        self.engine = None\n",
    "\n",
    "#    def create_engine(self):\n",
    "#        if self.engine is None:\n",
    "#            self.engine = create_engine(self._db_conf.conn_str)\n",
    "\n",
    "#        @event.listens_for(self.engine, 'before_cursor_execute')\n",
    "#        def receive_before_cursor_execute(conn, cursor, statement, params, context, executemany):\n",
    "#            if executemany:\n",
    "#                cursor.fast_executemany = True\n",
    "#                cursor.commit()\n",
    "\n",
    "#    @contextmanager\n",
    "#    def open_session(self):\n",
    "#        \"\"\"Provide a transactional scope around a series of operations.\"\"\"\n",
    "#        session: Session = sessionmaker(bind=self.engine)()\n",
    "#        try:\n",
    "#            yield session\n",
    "#            session.commit()\n",
    "#        except:\n",
    "#            session.rollback()\n",
    "#            raise\n",
    "#        finally:\n",
    "#            session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop('store_id', axis = 1).to_sql(name='' , con=db.engine, if_exists=\"append\", schema='', index=False,\n",
    "#                                    chunksize=10000)\n",
    "#print('database updated')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
